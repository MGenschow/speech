{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maltegenschow/.pyenv/versions/speech/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an audio dataset from .wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'musk',\n",
       " 'path': '../voice_cloning/voices/musk.wav',\n",
       " 'audio': {'path': '../voice_cloning/voices/musk.wav',\n",
       "  'array': array([-0.0107157 , -0.0207513 , -0.01781394, ..., -0.02346013,\n",
       "          0.08692658,  0.        ]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "files = glob('../voice_cloning/voices/*.wav')\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'name':[elem.split('/')[-1].split('.')[0] for elem in files],\n",
    "    'path':files,\n",
    "    'audio':files}).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'malte',\n",
       " 'path': '../voice_cloning/voices/malte.wav',\n",
       " 'audio': {'path': '../voice_cloning/voices/malte.wav',\n",
       "  'array': array([ 3.51592462e-06,  2.04406912e-04,  1.73166161e-04, ...,\n",
       "          1.62316253e-04, -1.63853336e-02, -3.93715128e-03]),\n",
       "  'sampling_rate': 16000}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maltegenschow/.pyenv/versions/speech/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/maltegenschow/.pyenv/versions/speech/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ich kann auch noch ein Beispiel Text von mir einsprechen, wo ich die ganze Zeit durchweg spreche und immer in der ungefähr gleichen Tunnelität und Tonlage und mal schauen ob das schon reicht.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 3\n",
    "display(dataset[id])\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "model.transcribe(dataset[id]['audio']['array'].astype('float32'))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I think if you're a leader and you don't understand the terms that you're using, that's probably the first start. It's really important that as a leader in the organisation you understand what digitisation means, you take the time to read widely in the sector. There are a lot of really good books. Kevin Kelly, who started Wide Magazine, has written a great book on various technologies. I think understanding the technologies, understanding what's out there so that you can separate the hype from the hope is really an important first step. And then making sure you understand the relevance of that for your function and how that fits into your business is the second step. I think two simple suggestions. One is I love the phrase brilliant at the basics, right? So how can you become brilliant at the basics? But beyond that, the fundamental thing I've seen which hasn't changed is so few organisations as a first step have truly taken control of their spend data. As a key first step on the digital transformation, taking ownership of data. And that's not a decision to use one vendor over someone else. That says we are going to be completely data driven. We're going to try and be as real time as possible. And we're going to be able to explain that data to anyone the way they want to see it. Understand why you're doing it. And then the second thing is reach out to suppliers in the market to talk to them, collaborate with them. You'll get a much better outcome. Think about what outcome you want at the end instead of thinking about the different processes and their software names. So, eSourcing being one of 20. Being big can be brave, I think, and talk to technology vendors because rather than just sending them forms, we won't bite you. I think we should fundamentally all of us rethink how procurement should be done and then start to define the functionality that we need and how we can make this work. What we're doing today is absolutely wrong. We don't like it, but people don't like it. Our colleagues don't like it. Nobody wants it. And we're spending a huge amount of money for no reason.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transcribe(\"audio/afjiv.wav\")['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "1. Speaker diarization\n",
    "2. Audio splitting based on speaker diarization\n",
    "3. Chunk transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.4.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maltegenschow/.pyenv/versions/speech/lib/python3.11/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/maltegenschow/.pyenv/versions/speech/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Users/maltegenschow/.pyenv/versions/speech/lib/python3.11/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Run Speaker diarization\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-2.1\")\n",
    "\n",
    "diarization = pipeline(\"audio/afjiv.wav\", num_speakers = 2)\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True,):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the audion file into speaker chunks\n",
    "from pydub import AudioSegment\n",
    "\n",
    "audio = AudioSegment.from_wav('audio/afjiv.wav')\n",
    "\n",
    "chunks = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    start_ms = int(turn.start * 1000)\n",
    "    end_ms = int(turn.end * 1000)\n",
    "    chunk_audio = audio[start_ms:end_ms]\n",
    "    chunk_audio.export(f\"{speaker}_{start_ms}_{end_ms}.wav\", format=\"wav\")\n",
    "    chunks.append({\n",
    "        'file': f\"splitted_speaker_files/{speaker}_{start_ms}_{end_ms}.wav\",\n",
    "        'speaker': speaker,\n",
    "        'start': turn.start,\n",
    "        'end': turn.end\n",
    "    })\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the actual transcribing using whisper\n",
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "final_transcript = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    result = model.transcribe(chunk['file'])\n",
    "    final_transcript.append({\n",
    "        'speaker': chunk['speaker'],\n",
    "        'start': chunk['start'],\n",
    "        'end': chunk['end'],\n",
    "        'text': result['text']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in final_transcript:\n",
    "    print(f\"[{entry['start']:.2f}–{entry['end']:.2f}] {entry['speaker']}: {entry['text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
